from transformers import AutoModel, AutoTokenizer
import torch
import os
import time

model_path = os.path.abspath(".")

print("Loading tokenizer and model...")
load_start = time.time()

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)

device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"Using {device.upper()} device")

model = AutoModel.from_pretrained(
    model_path,
    _attn_implementation="eager",
    trust_remote_code=True,
    local_files_only=True,
    use_safetensors=True,
)

model = model.eval().to(device).to(torch.bfloat16)
model.generation_config.max_new_tokens = 1024

load_time = time.time() - load_start
print(f"âœ… Model loaded in {load_time:.2f} seconds")

prompt = "<image>\n<|grounding|>Convert the document to markdown."
image_file = "/absolute/path/to/your/image.png"  # <- set this
output_path = "output_dir"

print("\n" + "="*50)
print("Starting OCR inference...")
print("="*50)
print(f"Image: {image_file}")
print(f"Device: {device}")
print("-"*50)

inference_start = time.time()
res = model.infer(
    tokenizer,
    prompt=prompt,
    image_file=image_file,
    output_path=output_path,
    base_size=640,
    image_size=640,
    crop_mode=False,
    save_results=True,
    test_compress=False,
)
inference_time = time.time() - inference_start

print("\n" + "="*50)
print("OCR COMPLETED!")
print("="*50)
print(f"â±ï¸  Model loading time: {load_time:.2f} seconds")
print(f"âš¡ Inference time: {inference_time:.2f} seconds ({inference_time/60:.2f} minutes)")
print(f"ðŸ“Š Total time: {load_time + inference_time:.2f} seconds ({(load_time + inference_time)/60:.2f} minutes)")
print(f"\nðŸ“ Output saved to: {output_path}/")
print("   - result.mmd (Markdown)")
print("   - result_with_boxes.jpg (Annotated image)")
print("="*50)
